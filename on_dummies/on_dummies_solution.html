<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Modeling groups with dummy variables &#8212; Functional MRI methods</title>
    
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/copybutton.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Functional MRI methods" href="../index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <p><span class="math">\(\newcommand{L}[1]{\| #1 \|}\newcommand{VL}[1]{\L{ \vec{#1} }}\newcommand{R}[1]{\operatorname{Re}\,(#1)}\newcommand{I}[1]{\operatorname{Im}\, (#1)}\)</span></p>
<div class="section" id="modeling-groups-with-dummy-variables">
<h1>Modeling groups with dummy variables<a class="headerlink" href="#modeling-groups-with-dummy-variables" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction-and-definitions">
<h2>Introduction and definitions<a class="headerlink" href="#introduction-and-definitions" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Import numerical and plotting libraries</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print to four digits of precision</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="kn">as</span> <span class="nn">npl</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</pre></div>
</div>
<p>We return to the psychopathy of students from Berkeley and MIT.</p>
<p>We get psychopathy questionnaire scores from another set of 5 students from
Berkeley:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Psychopathy scores from UCB students</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ucb_psycho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.9277</span><span class="p">,</span> <span class="mf">9.7348</span><span class="p">,</span> <span class="mf">12.1932</span><span class="p">,</span> <span class="mf">12.2576</span><span class="p">,</span> <span class="mf">5.4834</span><span class="p">])</span>
</pre></div>
</div>
<p>We do the same for another set of 5 students from MIT:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Psychopathy scores from MIT students</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mit_psycho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">7.2937</span><span class="p">,</span> <span class="mf">11.1465</span><span class="p">,</span> <span class="mf">13.5204</span><span class="p">,</span> <span class="mf">15.053</span><span class="p">,</span> <span class="mf">12.6863</span><span class="p">])</span>
</pre></div>
</div>
<p>Concatenate these into a <code class="docutils literal"><span class="pre">psychopathy</span></code> vector:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Concatenate UCB and MIT student scores</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">psychopathy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">ucb_psycho</span><span class="p">,</span> <span class="n">mit_psycho</span><span class="p">))</span>
</pre></div>
</div>
<p>We will use the general linear model to a two-level (UCB, MIT) single factor
(college) analysis of variance on these data.</p>
<p>Our model is that the Berkeley student data are drawn from some distribution
with a mean value that is characteristic for Berkeley: <span class="math">\(y_i = \mu_{Berkeley} +
e_i\)</span> where <span class="math">\(i\)</span> corresponds to a student from Berkeley.  There is also a
characteristic but possibly different mean value for MIT: <span class="math">\(\mu_{MIT}\)</span>:</p>
<div class="math">
\[ \begin{align}\begin{aligned}\newcommand{\yvec}{\vec{y}}
\newcommand{\xvec}{\vec{x}}
\newcommand{\evec}{\vec{\varepsilon}}
\newcommand{Xmat}{\boldsymbol X}
\newcommand{\bvec}{\vec{\beta}}
\newcommand{\bhat}{\hat{\bvec}}
\newcommand{\yhat}{\hat{\yvec}}\\y_i = \mu_{Berkeley} + e_i  \space\mbox{if}\space 1 \le i \le 5\\y_i = \mu_{MIT} + e_i \space\mbox{if}\space 6 \le i \le 10\end{aligned}\end{align} \]</div>
<p>We saw in <a class="reference external" href="https://matthew-brett.github.io/teaching/glm_intro.html">introduction to the general linear model</a> that we can encode this
group membership with dummy variables.  There is one dummy variable for each
group.  The dummy variables are <em>indicator</em> variables, in that they have 1 in
the row corresponding to observations in the group, and zero elsewhere.</p>
<p>We will compile a design matrix <span class="math">\(\Xmat\)</span> and use the matrix formulation of the
general linear model to do estimation and testing:</p>
<div class="math">
\[\yvec = \Xmat \bvec + \evec\]</div>
</div>
<div class="section" id="anova-design">
<h2>ANOVA design<a class="headerlink" href="#anova-design" title="Permalink to this headline">¶</a></h2>
<p>Create the design matrix for this ANOVA, with dummy variables corresponding to the UCB and MIT student groups:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Create design matrix for UCB / MIT ANOVA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">psychopathy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># UCB indicator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">[</span><span class="mi">5</span><span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># MIT indicator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[ 1.,  0.],</span>
<span class="go">       [ 1.,  0.],</span>
<span class="go">       [ 1.,  0.],</span>
<span class="go">       [ 1.,  0.],</span>
<span class="go">       [ 1.,  0.],</span>
<span class="go">       [ 0.,  1.],</span>
<span class="go">       [ 0.,  1.],</span>
<span class="go">       [ 0.,  1.],</span>
<span class="go">       [ 0.,  1.],</span>
<span class="go">       [ 0.,  1.]])</span>
</pre></div>
</div>
<p>Remember that, when <span class="math">\(\Xmat^T \Xmat\)</span> is invertible, our least-squares parameter
estimates <span class="math">\(\bhat\)</span> are given by:</p>
<div class="math">
\[\bhat = (\Xmat^T \Xmat)^{-1} \Xmat^T \yvec\]</div>
<p>First calculate <span class="math">\(\Xmat^T \Xmat\)</span>. Are the columns of this design orthogonal?</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate transpose of design with itself.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Are the design columns orthogonal?</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 5.,  0.],</span>
<span class="go">       [ 0.,  5.]])</span>
</pre></div>
</div>
<p>Calculate the inverse of <span class="math">\(\Xmat^T \Xmat\)</span>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate inverse of transpose of design with itself.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iXtX</span> <span class="o">=</span> <span class="n">npl</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iXtX</span>
<span class="go">array([[ 0.2,  0. ],</span>
<span class="go">       [ 0. ,  0.2]])</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>What is the relationship of the values on the diagonal of <span class="math">\((\Xmat^T
\Xmat)^{-1}\)</span> and the number of values in each group?</p>
<p>Answer: call the number of students in each group <span class="math">\(q\)</span>.  The diagonals of
<span class="math">\(\Xmat^T \Xmat\)</span> are, for each column <span class="math">\(\vec{w}\)</span>: <span class="math">\(\sum_i {w_i^2}\)</span>, which
reduces to <span class="math">\(q=5\)</span>, the number of ones in each column.  Because <span class="math">\(\Xmat^T
\Xmat\)</span> is diagonal, the inverse is:</p>
<div class="math">
\[\begin{split}(\Xmat^T \Xmat)^{-1} =
\begin{bmatrix}
\frac{1}{p} 0 \\
0 \frac{1}{p} \\
\end{bmatrix}\end{split}\]</div>
<p class="last">The diagonal values in <span class="math">\((\Xmat^T \Xmat)^{-1}\)</span> are therefore the reciprocal
of the number of values in each group.</p>
</div>
<p>Now calculate the second half of <span class="math">\((\Xmat^T \Xmat)^{-1} \Xmat^T \yvec\)</span>:
<span class="math">\(\vec{p} = \Xmat^T \yvec\)</span>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate transpose of design matrix multiplied by data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">XtY</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>What is the relationship of each element in this
vector to the values of <code class="docutils literal"><span class="pre">ucb_psycho</span></code> and <code class="docutils literal"><span class="pre">mit_psycho</span></code>?</p>
<p>The dot product of the dummy variables resolves to the sum of the values
for which the dummy vector value is 1 (and therefore not 0). Therefore the
values are just the sums of the values in <code class="docutils literal"><span class="pre">ucb_psycho</span></code> and
<code class="docutils literal"><span class="pre">mit_psycho</span></code> respectively:</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">XtY</span>
<span class="go">array([ 42.5967,  59.6999])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The apparent difference is just in the display of the numbers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="go">42.5966999...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="mi">5</span><span class="p">:])</span>
<span class="go">59.6999</span>
</pre></div>
</div>
</div>
<p>Now calculate <span class="math">\(\bhat\)</span> using <span class="math">\((\Xmat^T \Xmat)^{-1} \Xmat^T \yvec\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate beta vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span> <span class="o">=</span> <span class="n">iXtX</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XtY</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span>
<span class="go">array([  8.5193,  11.94  ])</span>
</pre></div>
</div>
<p>Compare this vector to the means of the values in <code class="docutils literal"><span class="pre">ucb_psycho</span></code> and
<code class="docutils literal"><span class="pre">mit_psycho</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Compare beta vector to means of each group</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ucb_psycho</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">8.51933...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mit_psycho</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">11.93998...</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>Using your knowledge of the parts of <span class="math">\((\Xmat^T \Xmat)^{-1} \Xmat^T \yvec\)</span>,
explain the relationship of the values in <span class="math">\(\bhat\)</span> to the means of
<code class="docutils literal"><span class="pre">ucb_psycho</span></code> and <code class="docutils literal"><span class="pre">mit_psycho</span></code>.</p>
<p>We found that <span class="math">\(\Xmat^T \yvec\)</span> contains the sums for <code class="docutils literal"><span class="pre">ucb_psych</span></code> and
<code class="docutils literal"><span class="pre">mit_psycho</span></code> respectively.  <span class="math">\((\Xmat^T \Xmat)^{-1}\)</span> is diagonal with
entries <span class="math">\(\frac{1}{q}\)</span> where <span class="math">\(q = 5\)</span> is the number of observations in each
group.  Therefore the entries in <span class="math">\(\bhat\)</span> are:</p>
<div class="math">
\[\frac{1}{q} \sum_i{v_i}\]</div>
<p class="last">for each vector <span class="math">\(\vec{v}\)</span> <code class="docutils literal"><span class="pre">ucb_psycho</span></code>, <code class="docutils literal"><span class="pre">mit_psycho</span></code>, which is also
the formula for the mean.</p>
</div>
</div>
<div class="section" id="hypothesis-testing-with-contrasts">
<h2>Hypothesis testing with contrasts<a class="headerlink" href="#hypothesis-testing-with-contrasts" title="Permalink to this headline">¶</a></h2>
<p>Remember the student&#8217;s t statistic from the general linear model <a class="footnote-reference" href="#col-vec" id="id1">[1]</a>:</p>
<div class="math">
\[\newcommand{\cvec}{\vec{c}}
t = \frac{\cvec^T \bhat}
{\sqrt{\hat{\sigma}^2 \cvec^T (\Xmat^T \Xmat)^+ \cvec}}\]</div>
<p>Let&#8217;s consider the top half of the t statistic, <span class="math">\(c^T \bhat\)</span>.</p>
<p>Our hypothesis is that the mean psychopathy score for MIT students,
<span class="math">\(\mu_{MIT}\)</span>, is higher than the mean psychopathy score for Berkeley students,
<span class="math">\(\mu_{Berkeley}\)</span>.  What contrast vector <span class="math">\(\cvec\)</span> do we need to apply to <span class="math">\(\bhat\)</span>
to express the difference between these means?  Apply this contrast vector to
<span class="math">\(\bhat\)</span> to get the top half of the t statistic.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Contrast vector to express difference between UCB and MIT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Resulting value will be high and positive when MIT students have</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#- higher psychopathy scores than UCB students</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">top_of_t</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">top_of_t</span>
<span class="go">3.42064...</span>
</pre></div>
</div>
<p>Now the bottom half of the t statistic.  Remember this is
<span class="math">\(\sqrt{\hat{\sigma}^2 \cvec^T (\Xmat^T \Xmat)^+ \cvec}\)</span>.</p>
<p>First we generate <span class="math">\(\hat{\sigma^2}\)</span> from the residuals of the model.</p>
<p>Calculate the fitted values and the residuals given the <span class="math">\(\bhat\)</span> that you have
already.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate the fitted and residual values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fitted</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">residuals</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">fitted</span>
</pre></div>
</div>
<p>Remember from <a class="reference external" href="https://github.com/bic-berkeley/psych-214-fall-2014/mean_test_example.html">worked example of GLM</a> that we want an unbiased estimator for
<span class="math">\(\sigma^2\)</span>, and therefore <span class="math">\(\sigma\)</span>.  For the case of a single regressor, this
involved dividing the sum of squares of the residuals by <span class="math">\(n - 1\)</span> where <span class="math">\(n\)</span> is
the number of rows in the design.  Now we can generalize this <span class="math">\(n - 1\)</span> measure
to designs with more than one column.  The general rule is that we divide the
sum of squares by <span class="math">\(n - m\)</span> where <span class="math">\(m\)</span> is the number of <em>independent</em> columns in
the design matrix.  Specifically, <span class="math">\(m\)</span> is the <a class="reference external" href="http://matthew-brett.github.io/teaching/matrix_rank.html">matrix rank</a> of the design
<span class="math">\(\Xmat\)</span>.  <span class="math">\(m\)</span> can also be called the <em>degrees of freedom</em> consumed by the
design.*  <span class="math">\(n - m\)</span> <em>is the *degrees of freedom of the error</em>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate the degrees of freedom consumed by the design</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">npl</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculated the degrees of freedom of the error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_error</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_error</span>
<span class="go">8</span>
</pre></div>
</div>
<p>Calculate the unbiased <em>variance</em> estimate <span class="math">\(\hat{\sigma^2}\)</span> by dividing the
sums of squares of the residuals by the degrees of freedom of the error.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate the unbiased variance estimate</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">df_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var_hat</span>
<span class="go">13.04946...</span>
</pre></div>
</div>
<p>Now the calculate second part of the t statistic denominator,  <span class="math">\(\cvec^T (\Xmat^T
\Xmat)^+ \cvec\)</span>. You already know that <span class="math">\(\Xmat^T \Xmat\)</span> is invertible, and you
have its inverse above, so you can use the inverse instead of the more general
pseudo-inverse.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate c (X.T X) c.T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c_iXtX_ct</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">npl</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c_iXtX_ct</span>
<span class="go">0.40000...</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>What is the relationship of <span class="math">\(\cvec^T (\Xmat^T \Xmat)^{-1} \cvec\)</span> to <span class="math">\(p\)</span>
– the number of observations in each group?</p>
<p>Answer: we already know that:</p>
<div class="math">
\[\begin{split}(\Xmat^T \Xmat)^{-1} =
\begin{bmatrix}
\frac{1}{p} 0 \\
0 \frac{1}{p} \\
\end{bmatrix}\end{split}\]</div>
<p>With contrast <span class="math">\(c = [-1, 1]\)</span> we get:</p>
<div class="last math">
\[\cvec^T (\Xmat^T \Xmat)^{-1} \cvec = \frac{2}{p}\]</div>
</div>
<p>Now, what is our t-value ?</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tstat</span> <span class="o">=</span> <span class="n">top_of_t</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_hat</span><span class="o">*</span><span class="n">c</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">iXtX</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
</pre></div>
</div>
<p>Is this significant ? use the stats model from scipy to create a t-distribution with
df_error degrees of freedom</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">sst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tdistrib</span> <span class="o">=</span> <span class="n">sst</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">df_error</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1 - cumulative density function (P(x &lt;= t)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.</span> <span class="o">-</span> <span class="n">tdistrib</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">tstat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># is the same as the &quot;survival function&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tdistrib</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">tstat</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>Now imagine your UCB and MIT are groups are not equal.  <span class="math">\(n\)</span> is constant,
the number of students. Call <span class="math">\(b\)</span> the number of Berkeley students in the
<span class="math">\(n=10\)</span>, where <span class="math">\(b \in [1, 2, ... 9]\)</span>.  Write the number of MIT students as
<span class="math">\(n - b\)</span>.  Using your answer above, derive a formula for the result of
<span class="math">\(\cvec^T (\Xmat^T \Xmat)^{-1} \cvec\)</span> in terms of <span class="math">\(b\)</span> and <span class="math">\(n\)</span>. <span class="math">\(\cvec\)</span> is
the contrast you chose above.  If all other things remain equal, such as
<span class="math">\(n = 10\)</span>, the <span class="math">\(\hat{\sigma^2}\)</span> and <span class="math">\(\cvec^T \bvec\)</span>, then which of the
possible values of <span class="math">\(b\)</span> should you chose to give the largest value for your
t statistic?</p>
<p>Answer: we now have:</p>
<div class="math">
\[\begin{split}(\Xmat^T \Xmat)^{-1} =
\begin{bmatrix}
\frac{1}{r} 0 \\
0 \frac{1}{n-r} \\
\end{bmatrix}\end{split}\]</div>
<p>With contrast <span class="math">\(c = [-1, 1]\)</span> we get:</p>
<div class="math">
\[\cvec^T (\Xmat^T \Xmat)^{-1} \cvec = \frac{1}{b} + \frac{1}{n-b}\]</div>
<p>To investigate, we make a Python function returning the result for a given
<code class="docutils literal"><span class="pre">b</span></code> and <code class="docutils literal"><span class="pre">n</span></code>, and evalulate for the possible values of <code class="docutils literal"><span class="pre">b</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">two_group_ct_ixtx_c</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">b</span> <span class="o">+</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">two_group_ct_ixtx_c</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>
<span class="go">array([ 1.1111,  0.625 ,  0.4762,  0.4167,  0.4   ,  0.4167,  0.4762,</span>
<span class="go">        0.625 ,  1.1111])</span>
</pre></div>
</div>
<p>We want <span class="math">\(\cvec^T (\Xmat^T \Xmat)^{-1} \cvec\)</span> to be small so that the t
value will be large.  So, all other things being equal, <span class="math">\(b = 5\)</span> will give
the largest t value.</p>
<p class="last">In general, for a fixed <span class="math">\(n\)</span>, we get the largest t statistic (and greatest
power) when comparing two groups of equal size.</p>
</div>
</div>
<div class="section" id="hypothesis-testing-f-tests">
<h2>Hypothesis testing: F-tests<a class="headerlink" href="#hypothesis-testing-f-tests" title="Permalink to this headline">¶</a></h2>
<p>T-test tests a linear combinaison of the <span class="math">\(\beta\)</span>, they would test if the mean of the first group is greater than the mean of the second group (<span class="math">\(\beta_1 - \beta_2\)</span>) or the opposite, but in any case these tests are <em>signed</em>.</p>
<p>In many instance, we do not know the direction of the test. Or we have to test the influence of several regressors on the data. In this cases, an F-test is more appropriate.</p>
<p>The simplest and generally most useful way of thinking of F test is to think as the test between two models: one which contains the regressor or factor that we want to test for (refered as the full model with design matrix <span class="math">\(X\)</span>), and one which doesnt (the reduced model <span class="math">\(X_0\)</span>). In the example above, what is our reduced model <span class="math">\(X_0\)</span> ?</p>
<p>The <em>reduced</em> model is the model here is simply a model where there is no difference between the group means: only the mean of the data is modelled, so, one column of <span class="math">\(n\)</span> values.</p>
<p>To test whether the model containing two columns is better, we compute the difference between the estimation of the noise variance between the models (variance estimated with X versus variance estimated with X0), normalized by the estimation of the noise (residual) variance under the full model <span class="math">\(X\)</span>. This is :</p>
<blockquote>
<div><div class="math">
\[\begin{split}\begin{eqnarray}
F_{\nu_1, \nu_2} &amp; = &amp; \frac{(\hat\epsilon_0^t \hat\epsilon_0 - \hat\epsilon^T\hat\epsilon)/ \nu_{1} }{\hat\epsilon^T\hat\epsilon/\nu_{2}} \\
&amp; = &amp; \frac{(\textrm{SSR}(X_0) - \textrm{SSR}(X))/\nu_1}{\textrm{SSR}(X)/\nu_2}
\end{eqnarray}\end{split}\]</div>
</div></blockquote>
<p>SSR here stands for &#8220;Sum of square of the residuals&#8221;.</p>
<p>What are <span class="math">\({\nu_1, \nu_2}\)</span>?</p>
<p><span class="math">\({\nu_2}\)</span> we have already encountered. This is the degrees of freedom of the <em>error</em>, that we have seen is <span class="math">\(n - 2\)</span>.</p>
<p>What is  <span class="math">\({\nu_1}\)</span> ? It&#8217;s something related. You remember that the degree of freedom of the residuals (the one we used to estimate the variance of the error) is <span class="math">\(n-m\)</span> with <span class="math">\(n\)</span> the number of observations, and <span class="math">\(m\)</span> the number of linearly independent columns in the design (the number of things to estimate). Here, we are looking at the difference between two designs, and this degrees of freedom will simply be this number <span class="math">\(m\)</span> minus the number of linearly independent columns in <span class="math">\(X_0\)</span>.</p>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>Make the alternative model <span class="math">\(X_0\)</span>. Compute the degrees of freedom <span class="math">\({\nu_1}\)</span>.
Compute the extra sum of squares and the F statistics. How is it related to
the t-statistics that you had above ? To investigate, create a little glm
function that takes as input X (design)  and Y (data) and returns Beta,
sum of square of the residuals, and degrees of freedom of the residuals.</p>
<p>Answer: we already know that <span class="math">\(\nu_2\)</span> == n-m</p>
<p>Compute the rank of <span class="math">\(X_0\)</span>: 1, rank of <span class="math">\(X\)</span>: 2
(this is 2), hence the numerator of the F statistics is <span class="math">\(\nu_1\)</span> == 2-1 == 1.</p>
<p>And the relation?  the F-statistics should be the square of the t-statistics.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">glm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="sd">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="sd">    input:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="sd">      X: design matrix, n observations times p columns</span>
<span class="gp">&gt;&gt;&gt; </span><span class="sd">      Y: data (n,1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="sd">    returns beta, residual_var, df_residual_var</span>
<span class="gp">&gt;&gt;&gt; </span><span class="sd">    &quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">B</span> <span class="o">=</span> <span class="n">npl</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">resid</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">df</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">npl</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">B</span><span class="p">,</span> <span class="n">resid</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">resid</span><span class="p">),</span> <span class="n">df</span>
<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<div class="last highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X0</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">rss</span><span class="p">,</span> <span class="n">df</span> <span class="o">=</span> <span class="n">glm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tstat</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">rss</span><span class="o">/</span><span class="n">df</span><span class="p">)</span><span class="o">*</span><span class="n">c</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">iXtX</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b0</span><span class="p">,</span> <span class="n">rss0</span><span class="p">,</span> <span class="n">df0</span> <span class="o">=</span> <span class="n">glm</span><span class="p">(</span><span class="n">X0</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nu1</span> <span class="o">=</span> <span class="n">m</span> <span class="o">-</span> <span class="n">npl</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Fstat</span> <span class="o">=</span> <span class="p">((</span><span class="n">rss0</span> <span class="o">-</span> <span class="n">rss</span><span class="p">)</span><span class="o">/</span><span class="n">nu1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">rss</span><span class="o">/</span><span class="n">df_error</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">tstat</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Fstat</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p class="rubric">Footnotes</p>
<table class="docutils footnote" frame="void" id="col-vec" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Assume the default that for any <span class="math">\(\vec{v}\)</span>, <span class="math">\(\vec{v}\)</span> is a
column vector, and therefore that <span class="math">\(\vec{v}^T\)</span> is a row vector.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">PSYCH 214 Fall 2016</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preparation.html">Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logistics.html">Logistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../classes.html">Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../labs.html">Labs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../projects.html">Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topics.html">Course material by topic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../example_data.html">Example datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>


<hr />
<ul>
    
    <li class="toctree-l1"><a href="_downloads">Website downloads</a></li>
    
    <li class="toctree-l1"><a href="https://nipy.bic.berkeley.edu/psych-214">Dataset downloads</a></li>
    
    <li class="toctree-l1"><a href="https://github.com/psych-214-fall-2016">Github organization</a></li>
    
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Matthew Brett, JB Poline.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="../_sources/on_dummies/on_dummies_solution.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>