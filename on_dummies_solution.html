<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Modeling groups with dummy variables exercise &mdash; Functional MRI methods</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/copybutton.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Functional MRI methods" href="index.html" />
    <link rel="next" title="Convolving with the hemodyamic response function" href="convolution_background.html" />
    <link rel="prev" title="Modeling groups with dummy variables exercise" href="on_dummies_exercise.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <p><span class="math">\(\newcommand{L}[1]{\| #1 \|}\newcommand{VL}[1]{\L{ \vec{#1} }}\newcommand{R}[1]{\operatorname{Re}\,(#1)}\newcommand{I}[1]{\operatorname{Im}\, (#1)}\)</span></p>
<div class="section" id="modeling-groups-with-dummy-variables-exercise">
<h1>Modeling groups with dummy variables exercise<a class="headerlink" href="#modeling-groups-with-dummy-variables-exercise" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction-and-definitions">
<h2>Introduction and definitions<a class="headerlink" href="#introduction-and-definitions" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Import numerical and plotting libraries</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print to four digits of precision</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="kn">as</span> <span class="nn">npl</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</pre></div>
</div>
<p>We return to the psychopathy of students from Berkeley and MIT.</p>
<p>We get psychopathy questionnaire scores from another set of 5 students from
Berkeley:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Psychopathy scores from UCB students</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ucb_psycho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.9277</span><span class="p">,</span> <span class="mf">9.7348</span><span class="p">,</span> <span class="mf">12.1932</span><span class="p">,</span> <span class="mf">12.2576</span><span class="p">,</span> <span class="mf">5.4834</span><span class="p">])</span>
</pre></div>
</div>
<p>We do the same for another set of 5 students from MIT:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Psychopathy scores from MIT students</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mit_psycho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">7.2937</span><span class="p">,</span> <span class="mf">11.1465</span><span class="p">,</span> <span class="mf">13.5204</span><span class="p">,</span> <span class="mf">15.053</span><span class="p">,</span> <span class="mf">12.6863</span><span class="p">])</span>
</pre></div>
</div>
<p>Concatenate these into a <code class="docutils literal"><span class="pre">psychopathy</span></code> vector:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Concatenate UCB and MIT student scores</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">psychopathy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">ucb_psycho</span><span class="p">,</span> <span class="n">mit_psycho</span><span class="p">))</span>
</pre></div>
</div>
<p>We will use the general linear model to a two-level (UCB, MIT) single factor
(college) analysis of variance on these data.</p>
<p>Our model is that the Berkeley student data are drawn from some distribution
with a mean value that is characteristic for Berkeley: <span class="math">\(y_i = \mu_{Berkeley} +
e_i\)</span> where <span class="math">\(i\)</span> corresponds to a student from Berkeley.  There is also a
characteristic but possibly different mean value for MIT: <span class="math">\(\mu_{MIT}\)</span>:</p>
<div class="math">
\[ \begin{align}\begin{aligned}\newcommand{\yvec}{\vec{y}}
\newcommand{\xvec}{\vec{x}}
\newcommand{\evec}{\vec{\varepsilon}}
\newcommand{Xmat}{\boldsymbol X}
\newcommand{\bvec}{\vec{\beta}}
\newcommand{\bhat}{\hat{\bvec}}
\newcommand{\yhat}{\hat{\yvec}}\\y_i = \mu_{Berkeley} + e_i  \space\mbox{if}\space 1 \le i \le 5\\y_i = \mu_{MIT} + e_i \space\mbox{if}\space 6 \le i \le 10\end{aligned}\end{align} \]</div>
<p>We saw in <a class="reference external" href="https://matthew-brett.github.io/teaching/glm_intro.html">introduction to the general linear model</a> that we can encode this
group membership with dummy variables.  There is one dummy variable for each
group.  The dummy variables are <em>indicator</em> variables, in that they have 1 in
the row corresponding to observations in the group, and zero elsewhere.</p>
<p>We will compile a design matrix <span class="math">\(\Xmat\)</span> and use the matrix formulation of the
general linear model to do estimation and testing:</p>
<div class="math">
\[\yvec = \Xmat \bvec + \evec\]</div>
</div>
<div class="section" id="anova-design">
<h2>ANOVA design<a class="headerlink" href="#anova-design" title="Permalink to this headline">¶</a></h2>
<p>Create the design matrix for this ANOVA, with dummy variables corresponding to the UCB and MIT student groups:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Create design matrix for UCB / MIT ANOVA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">psychopathy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># UCB indicator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">[</span><span class="mi">5</span><span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># MIT indicator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[ 1.,  0.],</span>
<span class="go">       [ 1.,  0.],</span>
<span class="go">       [ 1.,  0.],</span>
<span class="go">       [ 1.,  0.],</span>
<span class="go">       [ 1.,  0.],</span>
<span class="go">       [ 0.,  1.],</span>
<span class="go">       [ 0.,  1.],</span>
<span class="go">       [ 0.,  1.],</span>
<span class="go">       [ 0.,  1.],</span>
<span class="go">       [ 0.,  1.]])</span>
</pre></div>
</div>
<p>Remember that, when <span class="math">\(\Xmat^T \Xmat\)</span> is invertible, our least-squares parameter
estimates <span class="math">\(\bhat\)</span> are given by:</p>
<div class="math">
\[\bhat = (\Xmat^T \Xmat)^{-1} \Xmat^T \yvec\]</div>
<p>First calculate <span class="math">\(\Xmat^T \Xmat\)</span>. Are the columns of this design orthogonal?</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate transpose of design with itself.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Are the design columns orthogonal?</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 5.,  0.],</span>
<span class="go">       [ 0.,  5.]])</span>
</pre></div>
</div>
<p>Calculate the inverse of <span class="math">\(\Xmat^T \Xmat\)</span>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate inverse of transpose of design with itself.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iXtX</span> <span class="o">=</span> <span class="n">npl</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iXtX</span>
<span class="go">array([[ 0.2,  0. ],</span>
<span class="go">       [ 0. ,  0.2]])</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>What is the relationship of the values on the diagonal of <span class="math">\((\Xmat^T
\Xmat)^{-1}\)</span> and the number of values in each group?</p>
<p>Answer: call the number of students in each group <span class="math">\(q\)</span>.  The diagonals of
<span class="math">\(\Xmat^T \Xmat\)</span> are, for each column <span class="math">\(\vec{w}\)</span>: <span class="math">\(\sum_i {w_i^2}\)</span>, which
reduces to <span class="math">\(q=5\)</span>, the number of ones in each column.  Because <span class="math">\(\Xmat^T
\Xmat\)</span> is diagonal, the inverse is:</p>
<div class="math">
\[\begin{split}(\Xmat^T \Xmat)^{-1} =
\begin{bmatrix}
\frac{1}{p} &amp; 0 \\
0 &amp; \frac{1}{p} \\
\end{bmatrix}\end{split}\]</div>
<p class="last">The diagonal values in <span class="math">\((\Xmat^T \Xmat)^{-1}\)</span> are therefore the reciprocal
of the number of values in each group.</p>
</div>
<p>Now calculate the second half of <span class="math">\((\Xmat^T \Xmat)^{-1} \Xmat^T \yvec\)</span>:
<span class="math">\(\vec{p} = \Xmat^T \yvec\)</span>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate transpose of design matrix multiplied by data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">XtY</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>What is the relationship of each element in this
vector to the values of <code class="docutils literal"><span class="pre">ucb_psycho</span></code> and <code class="docutils literal"><span class="pre">mit_psycho</span></code>?</p>
<p>Answer: The dot product of the dummy variables resolves to the sum of the
values for which the dummy vector value is 1 (and therefore not 0).
Therefore the values are the sums of the values in <code class="docutils literal"><span class="pre">ucb_psycho</span></code> and
<code class="docutils literal"><span class="pre">mit_psycho</span></code> respectively:</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">XtY</span>
<span class="go">array([ 42.5967,  59.6999])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The apparent difference is just in the display of the numbers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="go">42.5966999...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="mi">5</span><span class="p">:])</span>
<span class="go">59.6999</span>
</pre></div>
</div>
</div>
<p>Now calculate <span class="math">\(\bhat\)</span> using <span class="math">\((\Xmat^T \Xmat)^{-1} \Xmat^T \yvec\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate beta vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span> <span class="o">=</span> <span class="n">iXtX</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XtY</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span>
<span class="go">array([  8.5193,  11.94  ])</span>
</pre></div>
</div>
<p>Compare this vector to the means of the values in <code class="docutils literal"><span class="pre">ucb_psycho</span></code> and
<code class="docutils literal"><span class="pre">mit_psycho</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Compare beta vector to means of each group</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ucb_psycho</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">8.51933...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mit_psycho</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">11.93998...</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>Using your knowledge of the parts of <span class="math">\((\Xmat^T \Xmat)^{-1} \Xmat^T \yvec\)</span>,
explain the relationship of the values in <span class="math">\(\bhat\)</span> to the means of
<code class="docutils literal"><span class="pre">ucb_psycho</span></code> and <code class="docutils literal"><span class="pre">mit_psycho</span></code>.</p>
<p>We found that <span class="math">\(\Xmat^T \yvec\)</span> contains the sums for <code class="docutils literal"><span class="pre">ucb_psych</span></code> and
<code class="docutils literal"><span class="pre">mit_psycho</span></code> respectively.  <span class="math">\((\Xmat^T \Xmat)^{-1}\)</span> is diagonal with
entries <span class="math">\(\frac{1}{q}\)</span> where <span class="math">\(q = 5\)</span> is the number of observations in each
group.  Therefore the entries in <span class="math">\(\bhat\)</span> are:</p>
<div class="math">
\[\frac{1}{q} \sum_i{v_i}\]</div>
<p class="last">for each vector <span class="math">\(\vec{v}\)</span> <code class="docutils literal"><span class="pre">ucb_psycho</span></code>, <code class="docutils literal"><span class="pre">mit_psycho</span></code>, which is also
the formula for the mean.</p>
</div>
</div>
<div class="section" id="hypothesis-testing-with-contrasts">
<h2>Hypothesis testing with contrasts<a class="headerlink" href="#hypothesis-testing-with-contrasts" title="Permalink to this headline">¶</a></h2>
<p>Remember the student&#8217;s t statistic from the general linear model <a class="footnote-reference" href="#col-vec" id="id1">[1]</a>:</p>
<div class="math">
\[\newcommand{\cvec}{\vec{c}}
t = \frac{\cvec^T \bhat}
{\sqrt{\hat{\sigma}^2 \cvec^T (\Xmat^T \Xmat)^+ \cvec}}\]</div>
<p>Let&#8217;s consider the top half of the t statistic, <span class="math">\(c^T \bhat\)</span>.</p>
<p>Our hypothesis is that the mean psychopathy score for MIT students,
<span class="math">\(\mu_{MIT}\)</span>, is higher than the mean psychopathy score for Berkeley students,
<span class="math">\(\mu_{Berkeley}\)</span>.  What contrast vector <span class="math">\(\cvec\)</span> do we need to apply to <span class="math">\(\bhat\)</span>
to express the difference between these means?  Apply this contrast vector to
<span class="math">\(\bhat\)</span> to get the top half of the t statistic.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Contrast vector to express difference between UCB and MIT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Resulting value will be high and positive when MIT students have</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#- higher psychopathy scores than UCB students</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">top_of_t</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">top_of_t</span>
<span class="go">3.42064...</span>
</pre></div>
</div>
<p>Now the bottom half of the t statistic.  Remember this is
<span class="math">\(\sqrt{\hat{\sigma}^2 \cvec^T (\Xmat^T \Xmat)^+ \cvec}\)</span>.</p>
<p>First we generate <span class="math">\(\hat{\sigma^2}\)</span> from the residuals of the model.</p>
<p>Calculate the fitted values and the residuals given the <span class="math">\(\bhat\)</span> that you have
already.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate the fitted and residual values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fitted</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">residuals</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">fitted</span>
</pre></div>
</div>
<p>We want an unbiased variance estimate for <span class="math">\(\hat\sigma^2\)</span>.  See the <a class="reference external" href="https://github.com/bic-berkeley/psych-214-fall-2014/mean_test_example.html">worked
example of GLM</a> page and the <a class="reference external" href="https://bic-berkeley.github.io/psych-214-fall-2016/hypothesis_tests.html#unbiased-variance">unbiased variance estimate</a> section for
details.</p>
<p>The general rule is that we divide the sum of squares by <span class="math">\(n - m\)</span> where <span class="math">\(m\)</span> is
the number of <em>independent</em> columns in the design matrix.  Specifically, <span class="math">\(m\)</span>
is the <a class="reference external" href="http://matthew-brett.github.io/teaching/matrix_rank.html">matrix rank</a> of the design <span class="math">\(\Xmat\)</span>.  <span class="math">\(m\)</span> can also be called the
<em>degrees of freedom of the design</em>.  <span class="math">\(n - m\)</span> is the <em>degrees of freedom of the
error</em> (see <a class="reference external" href="https://bic-berkeley.github.io/psych-214-fall-2016/hypothesis_tests.html#unbiased-variance">unbiased variance estimate</a>).</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate the degrees of freedom consumed by the design</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">npl</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate the degrees of freedom of the error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_error</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_error</span>
<span class="go">8</span>
</pre></div>
</div>
<p>Calculate the unbiased <em>variance</em> estimate <span class="math">\(\hat{\sigma^2}\)</span> by dividing the
sums of squares of the residuals by the degrees of freedom of the error.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate the unbiased variance estimate</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">df_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">var_hat</span>
<span class="go">13.04946...</span>
</pre></div>
</div>
<p>Now the calculate second part of the t statistic denominator,  <span class="math">\(\cvec^T (\Xmat^T
\Xmat)^+ \cvec\)</span>. You already know that <span class="math">\(\Xmat^T \Xmat\)</span> is invertible, and you
have its inverse above, so you can use the inverse instead of the more general
pseudo-inverse.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Calculate c (X.T X)^-1 c.T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c_iXtX_ct</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">npl</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c_iXtX_ct</span>
<span class="go">0.40000...</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>What is the relationship of <span class="math">\(\cvec^T (\Xmat^T \Xmat)^{-1} \cvec\)</span> to <span class="math">\(p\)</span>
– the number of observations in each group?</p>
<p>Answer: we already know that:</p>
<div class="math">
\[\begin{split}(\Xmat^T \Xmat)^{-1} =
\begin{bmatrix}
\frac{1}{p} &amp; 0 \\
0 &amp; \frac{1}{p} \\
\end{bmatrix}\end{split}\]</div>
<p>With contrast <span class="math">\(c = [-1, 1]\)</span> we get:</p>
<div class="last math">
\[\cvec^T (\Xmat^T \Xmat)^{-1} \cvec = \frac{2}{p}\]</div>
</div>
<p>Now, what is our t-value ?</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tstat</span> <span class="o">=</span> <span class="n">top_of_t</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_hat</span><span class="o">*</span><span class="n">c</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">iXtX</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tstat</span>
<span class="go">1.49720...</span>
</pre></div>
</div>
<p>Is this significant ? Use the <code class="docutils literal"><span class="pre">stats</span></code> module from <code class="docutils literal"><span class="pre">scipy</span></code> to create a
t-distribution with <code class="docutils literal"><span class="pre">df_error</span></code> (degrees of freedom of the error).  See the
<code class="docutils literal"><span class="pre">t_stat</span></code> function in <a class="reference external" href="https://matthew-brett.github.io/teaching/glm_intro.html">introduction to the general linear model</a> for
inspiration:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#- Use scipy.stats to test if your t-test value is significant.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">sst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tdistrib</span> <span class="o">=</span> <span class="n">sst</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">df_error</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1 - cumulative density function (P(x &lt;= t)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">1.</span> <span class="o">-</span> <span class="n">tdistrib</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">tstat</span><span class="p">)</span>
<span class="go">0.08635...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This is the same as the &quot;survival function&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tdistrib</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">tstat</span><span class="p">)</span>
<span class="go">0.08635...</span>
</pre></div>
</div>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>Now imagine your UCB and MIT groups are not of equal size.  The total
number of students <span class="math">\(n\)</span> has not changed. Call <span class="math">\(b\)</span> the number of Berkeley
students in the <span class="math">\(n=10\)</span>, where <span class="math">\(b \in [1, 2, ... 9]\)</span>.  Write the number of
MIT students as <span class="math">\(n - b\)</span>.  Using your reasoning for the case of equal group
sizes above, derive a simple mathematical formula for the result of
<span class="math">\(\cvec^T (\Xmat^T \Xmat)^{-1} \cvec\)</span> in terms of <span class="math">\(b\)</span> and <span class="math">\(n\)</span>. <span class="math">\(\cvec\)</span> is
the contrast you chose above.  If all other things remain equal, such as
<span class="math">\(n = 10\)</span>, the <span class="math">\(\hat{\sigma^2}\)</span> and <span class="math">\(\cvec^T \bhat\)</span>, then which of the
possible values of <span class="math">\(b\)</span> should you chose to give the largest value for your
t statistic?</p>
<p>Answer: we now have:</p>
<div class="math">
\[\begin{split}(\Xmat^T \Xmat)^{-1} =
\begin{bmatrix}
\frac{1}{b} &amp; 0 \\
0 &amp; \frac{1}{n-b} \\
\end{bmatrix}\end{split}\]</div>
<p>With contrast <span class="math">\(c = [-1, 1]\)</span> we get:</p>
<div class="math">
\[\cvec^T (\Xmat^T \Xmat)^{-1} \cvec = \frac{1}{b} + \frac{1}{n-b}\]</div>
<p>To investigate, we make a Python function returning the result for a given
<code class="docutils literal"><span class="pre">b</span></code> and <code class="docutils literal"><span class="pre">n</span></code>, and evalulate for the possible values of <code class="docutils literal"><span class="pre">b</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">two_group_ct_ixtx_c</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">b</span> <span class="o">+</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">two_group_ct_ixtx_c</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>
<span class="go">array([ 1.1111,  0.625 ,  0.4762,  0.4167,  0.4   ,  0.4167,  0.4762,</span>
<span class="go">        0.625 ,  1.1111])</span>
</pre></div>
</div>
<p>We want <span class="math">\(\cvec^T (\Xmat^T \Xmat)^{-1} \cvec\)</span> to be small so that the t
value will be large.  So, all other things being equal, <span class="math">\(b = 5\)</span> will give
the largest t value.</p>
<p>In general, for a fixed <span class="math">\(n\)</span>, we get the largest t statistic (and greatest
power) when comparing two groups of equal size.</p>
<p>We can prove this result by doing some calculus.  Here we are using the
<a class="reference external" href="http://sympy.org">sympy</a> Python package for computer algebra to show the results of the
differentiation and root finding:</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">sympy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">symbols</span><span class="p">(</span><span class="s1">&#39;n, b&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># construct the equation in terms of n and b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eqn</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># differentiate with respect to b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_db</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">eqn</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d_db</span>
<span class="go">(-b + n)**(-2) - 1/b**2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># find values of b at which differential is 0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sympy</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">d_db</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">[n/2]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="hypothesis-testing-f-tests">
<h2>Hypothesis testing: F-tests<a class="headerlink" href="#hypothesis-testing-f-tests" title="Permalink to this headline">¶</a></h2>
<p>Imagine we have also measured the clammy score for the Berkeley and MIT
students.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#: Clamminess of handshake for UCB and MIT students</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clammy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.6386</span><span class="p">,</span> <span class="mf">9.6094</span><span class="p">,</span> <span class="mf">8.3379</span><span class="p">,</span> <span class="mf">6.2871</span><span class="p">,</span> <span class="mf">7.2775</span><span class="p">,</span> <span class="mf">2.4787</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="mf">8.6037</span><span class="p">,</span> <span class="mf">12.8713</span><span class="p">,</span> <span class="mf">10.4906</span><span class="p">,</span> <span class="mf">5.6766</span><span class="p">])</span>
</pre></div>
</div>
<p>We want to test whether the clammy score is useful in explaining
the psychopathy data, over and above the students&#8217; college affiliation.</p>
<p>To do this, we will use an <a class="reference external" href="https://bic-berkeley.github.io/psych-214-fall-2016/hypothesis_tests.html#f-tests">F test</a>.</p>
<p>An F-test compares a <em>full model</em> <span class="math">\(\Xmat_f\)</span> with a <em>reduced model</em> <span class="math">\(\Xmat_r\)</span>.</p>
<p>In our case, <span class="math">\(\Xmat_f\)</span> is the model containing the <code class="docutils literal"><span class="pre">clammy</span></code> regressor, as
well as the two dummy columns for the UCB and MIT group means.</p>
<p><span class="math">\(\Xmat_r\)</span> is our original model, that only contains the dummy columns for the
UCB and MIT group means.</p>
<p>We define <span class="math">\(SSR(\Xmat_r)\)</span> and <span class="math">\(SSR(\Xmat_f)\)</span> as in <a class="reference external" href="https://bic-berkeley.github.io/psych-214-fall-2016/hypothesis_tests.html">hypothesis tests</a>.
These are the Sums of Squares of the Residuals of the reduced and full model
respectively.</p>
<div class="math">
\[ \begin{align}\begin{aligned}\begin{split}\bhat_r = \Xmat_r^+ \yvec \\
\hat\evec_r = \yvec - \Xmat_r \bhat_r \\
SSR(\Xmat_r) = \hat\evec_r^T \hat\evec_r \\\end{split}\\\begin{split}\bhat_f = \Xmat_f^+ \yvec \\
\hat\evec_f = \yvec - \Xmat_f \bhat_f \\
SSR(\Xmat_f) = \hat\evec_f^T \hat\evec_f\end{split}\end{aligned}\end{align} \]</div>
<p>You can calculate the F statistic for adding the <code class="docutils literal"><span class="pre">clammy</span></code> regressor, by
using these calculations and the formula for the F-test in <a class="reference external" href="https://bic-berkeley.github.io/psych-214-fall-2016/hypothesis_tests.html#f-tests">F tests</a>.</p>
<div class="admonition-question admonition">
<p class="first admonition-title">Question</p>
<p>Make the alternative full model <span class="math">\(\Xmat_f\)</span>. Compute the extra degrees of
freedom consumed by the design – <span class="math">\({\nu_1}\)</span>.  Compute the extra sum of
squares and the F statistic.</p>
<p>Answer: Construct <span class="math">\(\Xmat_f\)</span>; compute <span class="math">\(\nu_1\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Reduced design, and design matrix rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_r</span> <span class="o">=</span> <span class="n">X</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rank_r</span> <span class="o">=</span> <span class="n">npl</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X_r</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rank_r</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Full design, and design matrix rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">clammy</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rank_f</span> <span class="o">=</span> <span class="n">npl</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X_f</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rank_f</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nu_1</span> <span class="o">=</span> <span class="n">rank_f</span> <span class="o">-</span> <span class="n">rank_r</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nu_1</span>
<span class="go">1</span>
</pre></div>
</div>
<p>Make a function to compute <span class="math">\(\bhat, \textrm{SSR}(\Xmat), \nu_2\)</span> for a
given <span class="math">\(\Xmat\)</span> and <span class="math">\(\yvec\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">glm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="gp">... </span>    <span class="sd">&quot;&quot;&quot; Compute GLM on design `X` and data `Y`</span>
<span class="gp">...</span><span class="sd"></span>
<span class="gp">... </span><span class="sd">    Parameters</span>
<span class="gp">... </span><span class="sd">    ----------</span>
<span class="gp">... </span><span class="sd">    X : ndarray ahdpe (n, p)</span>
<span class="gp">... </span><span class="sd">        design matrix, n observations by p columns.</span>
<span class="gp">... </span><span class="sd">    Y : ndarray shape (n,) or (n,1)</span>
<span class="gp">... </span><span class="sd">        data.</span>
<span class="gp">...</span><span class="sd"></span>
<span class="gp">... </span><span class="sd">    Returns</span>
<span class="gp">... </span><span class="sd">    -------</span>
<span class="gp">... </span><span class="sd">    beta : ndarray shape (p,)</span>
<span class="gp">... </span><span class="sd">       estimated parameters for model `X`</span>
<span class="gp">... </span><span class="sd">    residual_ss : float</span>
<span class="gp">... </span><span class="sd">       sum of squares of residuals</span>
<span class="gp">... </span><span class="sd">    df_error : float</span>
<span class="gp">... </span><span class="sd">        Degrees of freedom due to error.</span>
<span class="gp">... </span><span class="sd">    &quot;&quot;&quot;</span>
<span class="gp">... </span>    <span class="n">B</span> <span class="o">=</span> <span class="n">npl</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">resid</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">df</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">npl</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">B</span><span class="p">,</span> <span class="n">resid</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">resid</span><span class="p">),</span> <span class="n">df</span>
</pre></div>
</div>
<p>Compute F statistic:</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">b_f</span><span class="p">,</span> <span class="n">rss_f</span><span class="p">,</span> <span class="n">df_f</span> <span class="o">=</span> <span class="n">glm</span><span class="p">(</span><span class="n">X_f</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b_r</span><span class="p">,</span> <span class="n">rss_r</span><span class="p">,</span> <span class="n">df_r</span> <span class="o">=</span> <span class="n">glm</span><span class="p">(</span><span class="n">X_r</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_stat</span> <span class="o">=</span> <span class="p">((</span><span class="n">rss_r</span> <span class="o">-</span> <span class="n">rss_f</span><span class="p">)</span> <span class="o">/</span> <span class="n">nu_1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">rss_f</span> <span class="o">/</span> <span class="n">df_f</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f_stat</span>
<span class="go">6.15949...</span>
</pre></div>
</div>
</div>
<p class="rubric">Footnotes</p>
<table class="docutils footnote" frame="void" id="col-vec" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Assume the default that for any <span class="math">\(\vec{v}\)</span>, <span class="math">\(\vec{v}\)</span> is a
column vector, and therefore that <span class="math">\(\vec{v}^T\)</span> is a row vector.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">PSYCH 214 Fall 2016</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="preparation.html">Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="logistics.html">Logistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes.html">Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="labs.html">Labs</a></li>
<li class="toctree-l1"><a class="reference internal" href="projects.html">Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="topics.html">Course material by topic</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercises.html">Exercises and homeworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_data.html">Example datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>


<hr />
<ul>
    
    <li class="toctree-l1"><a href="_downloads">Website downloads</a></li>
    
    <li class="toctree-l1"><a href="https://nipy.bic.berkeley.edu/psych-214">Dataset downloads</a></li>
    
    <li class="toctree-l1"><a href="https://github.com/psych-214-fall-2016">Github organization</a></li>
    
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="on_dummies_exercise.html" title="previous chapter">Modeling groups with dummy variables exercise</a></li>
      <li>Next: <a href="convolution_background.html" title="next chapter">Convolving with the hemodyamic response function</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Matthew Brett, JB Poline.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="_sources/on_dummies_solution.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>